{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary alert-info\">\n",
    "    \n",
    "# Reinforcement Learning - Mountain Car\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- ### SARSA ($\\lambda = 0$)\n",
    "\n",
    "- ### Q-Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "\n",
    "import gym\n",
    "\n",
    "import abc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/MountainCar_T0.gif'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(abc.ABC):\n",
    "\n",
    "    def __init__(self, env, num_bins, max_episodes):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.num_bins = num_bins\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_episode_length = 200\n",
    "\n",
    "        self.position_array = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=self.num_bins, endpoint=True)\n",
    "        self.velocity_array = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=self.num_bins, endpoint=True)\n",
    "\n",
    "        self.action_state_values = np.zeros((self.num_bins + 1, self.num_bins + 1, env.action_space.n))\n",
    "        self.policy = np.random.randint(low=0, high=env.action_space.n, size=(self.num_bins + 1, self.num_bins + 1))\n",
    "\n",
    "        self.eta = 0.05\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = 500 * self.epsilon_min / (self.max_episodes * self.max_episode_length)\n",
    "\n",
    "        \n",
    "    def digitize(self, obs):\n",
    "        return np.digitize(obs[0], self.position_array), np.digitize(obs[1], self.velocity_array)\n",
    "\n",
    "    \n",
    "    def select_action(self, obs):\n",
    "        digitized_obs = self.digitize(obs)\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay)\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return self.policy[digitized_obs]\n",
    "        else:\n",
    "            return np.random.choice(env.action_space.n)\n",
    "\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def learn(self, obs, action, reward, next_obs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### SARSA(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA_Agent(Agent):\n",
    "    \n",
    "    def learn(self, obs, action, reward, next_obs):\n",
    "        digitized_obs = self.digitize(obs)\n",
    "        digitized_next_obs = self.digitize(next_obs)\n",
    "        next_action = self.policy[digitized_next_obs]\n",
    "        action_values = reward + \\\n",
    "                        self.gamma * self.action_state_values[digitized_next_obs][next_action] - \\\n",
    "                        self.action_state_values[digitized_obs][action]\n",
    "        self.action_state_values[digitized_obs][action] += self.eta * action_values\n",
    "        self.policy[digitized_obs] = np.argmax(self.action_state_values[digitized_obs])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent(Agent):\n",
    "    \n",
    "    def learn(self, obs, action, reward, next_obs):\n",
    "        digitized_obs = self.digitize(obs)\n",
    "        digitized_next_obs = self.digitize(next_obs)\n",
    "        action_values = reward + \\\n",
    "                        self.gamma * np.max(self.action_state_values[digitized_next_obs]) - \\\n",
    "                        self.action_state_values[digitized_obs][action]\n",
    "        self.action_state_values[digitized_obs][action] += self.eta * action_values\n",
    "        self.policy[digitized_obs] = np.argmax(self.action_state_values[digitized_obs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent):\n",
    "    best_reward = np.finfo(np.float64).min\n",
    "    for episode_idx in range(agent.max_episodes):\n",
    "        done, reward_per_episode = False, 0.0\n",
    "        obs = agent.env.reset()\n",
    "        while not done:\n",
    "            action = agent.select_action(obs)\n",
    "            next_obs, reward, done, info = agent.env.step(action)\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "            obs = next_obs\n",
    "            reward_per_episode += reward\n",
    "        best_reward = max(best_reward, reward_per_episode)\n",
    "        if episode_idx % 10000 == 0:\n",
    "            print(f'Episode: {episode_idx+1}, Reward: {reward_per_episode}, Best_reward: {best_reward}')\n",
    "    print(f'Episode: {agent.max_episodes}, Reward: {reward_per_episode}, Best_reward: {best_reward}\\n')\n",
    "    return np.argmax(agent.action_state_values, axis=2)\n",
    "\n",
    "\n",
    "def test(agent, env, policy):\n",
    "    done, total_reward = False, 0.0\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = policy[agent.digitize(obs)]\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: -200.0, Best_reward: -200.0\n",
      "Episode: 10001, Reward: -200.0, Best_reward: -200.0\n",
      "Episode: 20001, Reward: -200.0, Best_reward: -159.0\n",
      "Episode: 30001, Reward: -200.0, Best_reward: -118.0\n",
      "Episode: 40001, Reward: -156.0, Best_reward: -86.0\n",
      "Episode: 50001, Reward: -141.0, Best_reward: -86.0\n",
      "Episode: 60001, Reward: -141.0, Best_reward: -86.0\n",
      "Episode: 70001, Reward: -147.0, Best_reward: -86.0\n",
      "Episode: 80001, Reward: -144.0, Best_reward: -86.0\n",
      "Episode: 90001, Reward: -144.0, Best_reward: -86.0\n",
      "Episode: 100000, Reward: -140.0, Best_reward: -86.0\n",
      "\n",
      "Episode: 1, Reward: -200.0, Best_reward: -200.0\n",
      "Episode: 10001, Reward: -200.0, Best_reward: -200.0\n",
      "Episode: 20001, Reward: -200.0, Best_reward: -149.0\n",
      "Episode: 30001, Reward: -153.0, Best_reward: -117.0\n",
      "Episode: 40001, Reward: -149.0, Best_reward: -88.0\n",
      "Episode: 50001, Reward: -174.0, Best_reward: -88.0\n",
      "Episode: 60001, Reward: -189.0, Best_reward: -86.0\n",
      "Episode: 70001, Reward: -150.0, Best_reward: -86.0\n",
      "Episode: 80001, Reward: -152.0, Best_reward: -86.0\n",
      "Episode: 90001, Reward: -112.0, Best_reward: -86.0\n",
      "Episode: 100000, Reward: -160.0, Best_reward: -86.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    env = gym.make('MountainCar-v0')\n",
    "\n",
    "    max_episodes = np.arange(10_000, 100_001, 10_000, dtype=np.int32)\n",
    "    num_bins = np.arange(20, 40, 5, dtype=np.int32)\n",
    "    \n",
    "    \n",
    "    # SARSA Agent\n",
    "    agent = SARSA_Agent(env, num_bins[0], max_episodes[-1])\n",
    "    policy = train(agent)\n",
    "\n",
    "    output_dir = './sarsa_output'\n",
    "    env = gym.wrappers.Monitor(env, output_dir, force=True)\n",
    "    for _ in range(1000):\n",
    "        test(agent, env, policy)\n",
    "\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    \n",
    "    # Q Agent\n",
    "    agent = Q_Agent(env, num_bins[0], max_episodes[-1])\n",
    "    policy = train(agent)\n",
    "\n",
    "    output_dir = './q_output'\n",
    "    env = gym.wrappers.Monitor(env, output_dir, force=True)\n",
    "    for _ in range(1000):\n",
    "        test(agent, env, policy)\n",
    "\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "### Q Agent\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### During training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/Q_Agent_Intermediate.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### After training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/Q_Agent.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "### SARSA Agent\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### During training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/SARSA_Agent_Intermediate.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### After training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/SARSA_Agent.gif'/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
